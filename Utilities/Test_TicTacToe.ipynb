{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from ReplayBuffer import PrioritizedReplayBuffer\n",
    "from TicTacToe import TicTacToeEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.001\n",
    "epsilon_max = 1.0\n",
    "epsilon_interval = (epsilon_max - epsilon_min)\n",
    "batch_size = 32\n",
    "num_actions = 9\n",
    "\n",
    "env = TicTacToeEnv()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_q_model():\n",
    "    inputs = keras.layers.Input(shape=(3,3,1,))\n",
    "    #layer1 = keras.layers.Conv2D(64, 2, strides=1, activation=\"relu\")(inputs)\n",
    "    #layer2 = keras.layers.Conv2D(32, 2, activation=\"relu\")(layer1)\n",
    "\n",
    "    #layer4 = keras.layers.Flatten()(layer2)\n",
    "    layer4 = keras.layers.Dense(64, activation=\"relu\")(inputs)\n",
    "\n",
    "    layer5 = keras.layers.Dense(32, activation=\"relu\")(layer4)\n",
    "    layer6 = keras.layers.Flatten()(layer5)\n",
    "    layer7 = keras.layers.Dense(16, activation=\"relu\")(layer6)\n",
    "    out = keras.layers.Dense(num_actions, activation=\"linear\")(layer7)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=out)\n",
    "\n",
    "model = create_q_model()\n",
    "model_target = create_q_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "frame_count = 0\n",
    "epsilon_random_frames = 5000\n",
    "epsilon_greedy_frames = 200000\n",
    "update_after_actions = 4\n",
    "# How often to update the target network\n",
    "update_target_network = 10000\n",
    "# Using huber loss for stability\n",
    "loss_function = keras.losses.Huber()\n",
    "memory = PrioritizedReplayBuffer(8192, (3, 3, 1), 1, 0.7)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self):\n",
    "        self.pos = [0,0]\n",
    "        self.exit_pole = [0,0,0,0]\n",
    "        self.total_turns = 0\n",
    "        self.first = True\n",
    "        self.total_turns = 0\n",
    "        self.score = 0\n",
    "        self.turns = 0\n",
    "        self.num_games = 2\n",
    "\n",
    "    def reset(self):\n",
    "        self.pos = [0,0]\n",
    "        self.exit_pole = [0,0,0,0]\n",
    "        self.total_turns = 0\n",
    "        self.first = True\n",
    "        self.total_turns = 0\n",
    "        self.score = 0\n",
    "        self.turns = 0\n",
    "        self.num_games = 2\n",
    "\n",
    "    def update_stats(self):\n",
    "        self.total_turns += self.turns\n",
    "        if self.score == -2 or self.score == 0:\n",
    "            self.exit_pole[self.score] += 1\n",
    "        elif self.first:\n",
    "            self.exit_pole[self.score] += 1\n",
    "        else:\n",
    "            self.exit_pole[-self.score] += 1\n",
    "        self.pos[self.first] += 1\n",
    "        self.first = not self.first\n",
    "\n",
    "    def play_net_turn(self, baby, test_env, init_state):\n",
    "        action_p = baby(tf.expand_dims(init_state, 0), training=False)\n",
    "        act = tf.argmax(action_p[0]).numpy()\n",
    "        next_state, rw, terminal, info = test_env.step(act)\n",
    "        self.score += rw\n",
    "        self.turns+=1\n",
    "        print()\n",
    "        print(np.squeeze(next_state,axis=-1), info, act)\n",
    "        print()\n",
    "        return next_state, rw, terminal, info\n",
    "\n",
    "    def play_minmax_turn(self, test_env):\n",
    "        act = test_env.minimax(test_env.state)\n",
    "        next_state, rw, terminal, info = test_env.step(act)\n",
    "        self.score += rw\n",
    "        self.turns+=1\n",
    "        print()\n",
    "        print(np.squeeze(next_state,axis=-1), info, act)\n",
    "        print()\n",
    "        return next_state, rw, terminal, info\n",
    "\n",
    "    def display_stats(self):\n",
    "        print('##################################################')\n",
    "        print('Mean Game turns: ', self.total_turns/self.num_games)\n",
    "        print('Invalid: ', self.exit_pole[-2])\n",
    "        print('Win: ', self.exit_pole[1])\n",
    "        print('Tie: ', self.exit_pole[0])\n",
    "        print('Lose: ', self.exit_pole[-1])\n",
    "        print('##################################################')\n",
    "\n",
    "    def play_full_game(self, baby):\n",
    "        print('**************************************************')\n",
    "        self.reset()\n",
    "        for i in range(self.num_games):\n",
    "            test_env = TicTacToeEnv()\n",
    "            next_state = test_env.reset()\n",
    "            terminal = False\n",
    "            self.turns = 0\n",
    "            self.score = 0\n",
    "            while not terminal:\n",
    "                if self.first:\n",
    "                    next_state, rw, terminal, info =  self.play_net_turn(baby, test_env, next_state)\n",
    "                    if terminal:\n",
    "                        self.update_stats()\n",
    "                        break\n",
    "                    next_state, rw, terminal, info =  self.play_minmax_turn(test_env)\n",
    "                    if terminal:\n",
    "                        self.update_stats()\n",
    "                        break\n",
    "                else:\n",
    "                    next_state, rw, terminal, info =  self.play_minmax_turn(test_env)\n",
    "                    if terminal:\n",
    "                        self.update_stats()\n",
    "                        break\n",
    "                    next_state, rw, terminal, info =  self.play_net_turn(baby, test_env, next_state)\n",
    "                    if terminal:\n",
    "                        self.update_stats()\n",
    "                        break\n",
    "        self.display_stats()\n",
    "        print('**************************************************')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "episode_reward_history = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eval_tool = Evaluator()\n",
    "while frame_count < epsilon_greedy_frames:\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    rewards_sample = None\n",
    "    while True:\n",
    "        frame_count += 1\n",
    "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
    "            action = np.random.choice(num_actions)\n",
    "        else:\n",
    "            state_tensor = tf.expand_dims(state, 0)\n",
    "            action_probs = model(state_tensor, training=False)\n",
    "            action = tf.argmax(action_probs[0]).numpy()\n",
    "\n",
    "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "        state_next, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        memory.push(state, action, reward, state_next, done)\n",
    "        if not done:\n",
    "            state, reward, done, _ = env.step(env.minimax(env.state))\n",
    "            episode_reward += reward\n",
    "            memory.push(-state, action, -reward, -state_next, done)\n",
    "        if frame_count % update_after_actions == 0 and frame_count > batch_size:\n",
    "            state_sample, action_sample, rewards_sample, state_next_sample, done_sample, weights, indexes = memory.pop(batch_size, 0.4)\n",
    "            future_rewards = model_target.predict(state_next_sample)\n",
    "            updated_q_values = rewards_sample + gamma * tf.reduce_max(\n",
    "                future_rewards, axis=1\n",
    "            )\n",
    "            updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
    "            actions = tf.squeeze(tf.cast(action_sample, dtype=tf.int32), axis=-1)\n",
    "            masks = tf.one_hot(actions, num_actions)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                q_values = model(state_sample)\n",
    "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "                loss = loss_function(updated_q_values, q_action, sample_weight=tf.expand_dims(weights, axis=-1))\n",
    "\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            memory.update_priorities_variant(indexes, tf.math.abs(updated_q_values - q_action))\n",
    "\n",
    "        if frame_count % update_target_network == 0:\n",
    "            model_target.set_weights(model.get_weights())\n",
    "\n",
    "        if frame_count % 5000 == 0:\n",
    "            eval_tool.play_full_game(model)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    if rewards_sample is not None:\n",
    "        print(frame_count, episode_reward, epsilon, sum(rewards_sample))\n",
    "\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    if len(episode_reward_history) > 100:\n",
    "        del episode_reward_history[:1]\n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "    episode_count += 1\n",
    "\n",
    "    if running_reward == 0 and len(episode_reward_history) == 100:  # Condition to consider the task solved\n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-e1239252",
   "language": "python",
   "display_name": "PyCharm (Advanced_Deep_Learning_Models_and_Methods-Menta-Nisti)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}